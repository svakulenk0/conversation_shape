{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to /home/zola/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7935\n",
      "14 distinct classes:\n",
      "['Statement', 'Emotion', 'Greet', 'Accept', 'Reject', 'whQuestion', 'Continuer', 'ynQuestion', 'yAnswer', 'Bye', 'Clarify', 'Emphasis', 'nAnswer', 'Other']\n"
     ]
    }
   ],
   "source": [
    "# use NPS Chat Corpus dataset from nltk\n",
    "# that consists of posts from instant messaging sessions. These posts have all been labeled with one of 15 dialogue act types, such as \"Statement,\" \"Emotion,\" \"ynQuestion\", and \"Continuer.\"\n",
    "# to train an intent classifier identifying questions\n",
    "# code based on https://stackoverflow.com/questions/49100615/nltk-detecting-whether-a-sentence-is-interogative-or-not/50583762\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('nps_chat')\n",
    "posts = nltk.corpus.nps_chat.xml_posts()  # [:10000]\n",
    "\n",
    "# encode labels as int\n",
    "classes = []\n",
    "df = pd.DataFrame(columns=['text', 'labels'])\n",
    "for post in posts:\n",
    "    c = post.get('class')\n",
    "    if c not in ['System']:\n",
    "        if c not in classes:\n",
    "            classes.append(c)\n",
    "        df = df.append({'text': post.text, 'labels': classes.index(c)}, ignore_index=True)\n",
    "\n",
    "print(len(df))\n",
    "# check classes\n",
    "print(\"%d distinct classes:\"%len(classes))\n",
    "print(classes)\n",
    "\n",
    "# prepare the dataset for training classifier\n",
    "size = int(len(df) * 0.1)\n",
    "train_set, test_set = df[size:], df[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7142\n",
      "2\n",
      "text      sexy is just a bonus at this point in my life....\n",
      "labels                                                    0\n",
      "Name: 793, dtype: object\n",
      "Statement\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(train_set.columns))\n",
    "\n",
    "print(train_set.iloc[0])\n",
    "print(classes[train_set.iloc[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1228 01:34:27.127121 140553850861376 file_utils.py:35] PyTorch version 1.0.1.post2 available.\n",
      "I1228 01:34:27.951721 140553850861376 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /home/zola/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.9dad9043216064080cf9dd3711c53c0f11fe2b09313eaa66931057b4bdcaf068\n",
      "I1228 01:34:27.954186 140553850861376 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 14,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I1228 01:34:28.419760 140553850861376 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /home/zola/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I1228 01:34:31.151742 140553850861376 modeling_utils.py:480] Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I1228 01:34:31.152215 140553850861376 modeling_utils.py:483] Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "I1228 01:34:32.086787 140553850861376 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /home/zola/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I1228 01:34:32.088601 140553850861376 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /home/zola/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "# fine-tune a pre-trained transformer for intent classification task\n",
    "# using simple transformers library https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('roberta', 'roberta-base', num_labels=len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded from cache at cache_dir/cached_train_roberta_128_14_7142\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089e9e2996144616bbf5389e06859dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3006630ebf49dd96ad9d26a012b431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Current iteration', max=893, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.784051Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Running loss: 2.131428Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Running loss: 1.818022Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Running loss: 0.926366Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Running loss: 0.980025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1228 01:38:34.068494 140553850861376 configuration_utils.py:87] Configuration saved in outputs/epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.842740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1228 01:38:34.393338 140553850861376 modeling_utils.py:258] Model weights saved in outputs/epoch-1/pytorch_model.bin\n",
      "I1228 01:38:34.470797 140553850861376 configuration_utils.py:87] Configuration saved in outputs/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1228 01:38:35.601099 140553850861376 modeling_utils.py:258] Model weights saved in outputs/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of roberta model complete. Saved to outputs/.\n",
      "Features loaded from cache at cache_dir/cached_dev_roberta_128_14_793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3e6a758815418a9f1e795d7232a289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train_model(train_set)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "    \n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_set, f1=f1_multiclass, acc=accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mcc': 0.7508701664513905, 'f1': 0.8095838587641866, 'acc': 0.8095838587641866, 'eval_loss': 0.6986473923921586}\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "# roberta {'mcc': 0.7508701664513905, 'f1': 0.8095838587641866, 'acc': 0.8095838587641866, 'eval_loss': 0.6986473923921586}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to features started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52233f8219c44bc870cb73f892ddb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140cb9846f224eff93789a28c2742ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statement\n",
      "whQuestion\n",
      "ynQuestion\n",
      "ynQuestion\n",
      "Accept\n",
      "nAnswer\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict(['Some arbitary sentence', 'where is he from?', 'would you agree?', 'is this true?', 'yeap', 'not'])\n",
    "for p in predictions:\n",
    "    print(classes[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch36",
   "language": "python",
   "name": "torch36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
